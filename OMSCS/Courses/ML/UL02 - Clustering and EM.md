---
tags:
  - OMSCS
  - ML
---
# UL02 - Clustering & EM
- supervised learning
	- function approximation
	- use labeled training data to generalize labels to new instances
- unsupervised learning
	- data description
	- make sense out of unlabeled data

## Basic Clustering Problem
Given
- set of objects $X$
- inter-object distances 
	- $D(x,y)=D(y,x)$
	- $x\in X$, $y\in X$
Output
- Partition $P_D(x)=P_D(y)$
- if $x$ and $y$ in the same cluster
Examples
- $\forall_x P_D(x)=1$
- $\forall_x P_D(x)=x$

## Simple Linkage Clustering (SLC)
- It's a Hierarchical Agglomerative Clustering algorithm (HAC)
- consider each object a cluster ($n$ objects)
- define intercluster distance as the distance between the two closest points in the tho clusters
- merge two closest clusters
- repeat $n-k$ times to make $k$ clusters
- Requires many inter-cluster distance function evaluations
	- [[simple_clustering_example.py]]
	- $n-k:$ number of algorithm repetitions
	- $\frac{1}{2}(n^2-n):$ triangle numbers, determines the number of comparisons done in the first algorithm iteration
	- $-\frac{1}{6}\left((n-k-1)*(n-k)*(n-k+1)\right):$ tetrahedral numbers, correction factor determined experimentally.

$$
(n-k)\frac{n^2-n}{2}-\left(\frac{(n-k-1)*(n-k)*(n-k+1)}{6}\right)
$$

Alternate forms. The last form shows that the algorithm is $O(N^3)$ in terms of the number of distance function evaluations that need to be performed. For $|X|=1000$ and $k=2$, this requires $332,832,501$ distance function evaluations.

$$
(n-k)\frac{3n^2-3n}{6}-(n-k)\frac{k^2 - 2 k n + n^2 - 1}{6}
$$
$$
(n-k)\left(\frac{3n^2-3n}{6}-\frac{k^2 - 2 k n + n^2 - 1}{6}\right)
$$
$$
\frac{n-k}{6}\left({2n^2-3n - k^2 + 2 k n + 1}\right)
$$
$$
\frac{1}{3}n^3 - \frac{1}{2}n^2 + \frac{1}{6}n + \frac{1}{6}k^3 - \frac{1}{2}k^2 n + \frac{1}{2}k n - \frac{1}{6} k 
$$

![[Pasted image 20250308144107.png]]

## K-Means Clustering
- pick K centers at random
- each center "claims" its closest points
- recompute the centers by averaging the clustered points
- repeat until convergence
- See: [[Module 7 - Machine Learning]]

### K-means in Euclidean Space
- $P^t(X):$ partition/cluster of object X at time step (iteration) $t$
- $C_i^t:$ set of all points in cluster $i$ at iteration $t$. $\left\{\text{x s.t. }P(x)=i\right\}$
- $center_i^t=\sum_{y\in C_i^t}y/|C_i|$ (centroid of cluster $i$ at iteration $t$)

![[Pasted image 20250308212143.png]]

### K-means as optimization
- configurations
	- center
	- P
- scores
	- Error: $E(P, center)=\sum_x||center_{P(x)}-x||_2^2$
- neighborhood

![[Pasted image 20250308212419.png]]

It's essentially a hill-climbing algorithm. At east time step, it tries to improves the clustering.

- The distance of a point to its closest cluster centroid can only go down.
- Taking the average location of all points in each cluster minimizes squared error.
- The centers exist in an infinite space, but the centers are based on the classifications of discrete points in that space, so there are a finite number of configurations.
- Ties should be broken consistently (always pick the lowest $i$ ID if distances are the same)

### Properties of K-means clustering
- each iteration is polynomial: $O(kn)$
- finite (exponential) iterations: $O(k^n)$
	- in practice K-means converges quite quickly
- error decreases if ties are broken consistently (with one exception)
- can get stuck!
- Bad initial clusters can cause poor resulting clusters.
	- It's hill climbing.
	- Random restarts help.
	- Picking initial centers that are far away from each other can help.

![[Pasted image 20250308213658.png]]

## Soft Clustering

![[Pasted image 20250308213816.png]]

In K-means, K should clearly be 3, but when K=2, $d$ sometimes d ends up either on the left or right cluster, depending on the precision of this drawing and depending on the random starting state.

For soft clustering, assume the data was generated by:
1. Select one of K gaussians (fixed known variance $\sigma^2$) uniformly
2. sample $x_i$ from that gaussian
3. repeat $n$ times 

Task: Find a hypothesis $h=\langle\mu_1,...,\mu_k\rangle$ (a collection of K means, not to be confused with k-means) that maximizes the probability of the data. This is a Maximum Likelihood (ML) algorithm.

### Maximum Likelihood (ML) Gaussian
- The ML mean of the gaussian $\mu$ is the mean of the data.
- What if there's K of them? Hidden variables!
- $\langle X, Z_1, Z_2, ..., Z_k \rangle$
- Adding these extra hidden variables helps break up the problem

## Expectation Maximization (EM)
![[Pasted image 20250308214640.png]]

- Similar algorithm to K-means
- Left is ML / Expectation
	- Likelihood that element $i$ came from cluster $j$
	- Defining $Z$ variables from $\mu$
- Right is Maximization
	- Take the average variable value from within cluster $j$
	- Possible to have fractional datapoints. Weighted averages.
- End up with exactly K-means if you force probabilities to be 0 or 1.
- Allows points to have "low probability" of being part of all but one cluster, with high probability.
- Also allows points to have low probability of being part of any cluster over another, which can happen on the decision boundary between adjacent clusters.

### Properties of EM
- monotonically non-decreasing likelihood
- does not converge (practically does)
- will not diverge
- can get stuck
- works with any distribution (if E and M steps are solvable)
- also has local optimum problems
	- random restarts can help

## Clustering Properties
- Richness
	- For any assignment of objects to clusters, there is some distance matrix $D$ such that $P_D$ returns that clustering.
- Scale-invariance
	- Scaling distances by a positive value does not change the clustering. (unit conversion for example)
- Consistency
	- Shrinking intracluster distances and expanding the intercluster distances does not change the clustering: $P_D=P_{D'}$

![[Pasted image 20250308220418.png]]

This is the Impossibility Theorem of clustering algorithms. No clustering scheme can achieve richness, scale invariance, and consistency.

Clustering can be used to help you get to know your data better. It's also superb when you already know approximately how many processes produced your data set, i.e. you already have good guesses for $K$ (i.e. you don't need richness).

