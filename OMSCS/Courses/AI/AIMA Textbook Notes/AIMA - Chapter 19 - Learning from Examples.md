---
tags:
  - OMSCS
  - AI
  - AIMA
  - ML
---
# AIMA - Chapter 19 - Learning from Examples

(These notes are incomplete to the chapter. The course broke this chapter up into sections for this week.)

## 19.1 - Forms of Learning
> Any component of an agent program can be improved by machine learning.

The techniques used depend on these factors
- Which components do we want to improve
- What prior knowledge the agent has, which influences the model it builds
- What data and feedback on that data are available

Components of agents include
1. direct mapping from conditions on the current state to actions
2. a means to infer relevant properties of the world from the percept sequence
3. information about the way the world evolves and about the results of possible actions the agent can take
4. utility information indicating the desirability of world states
5. action-value information indicating the desirability of actions
6. goals that describe the most desirable states
7. a problem generator, critic, and learning element that enable the system to improve

Key terms for this chapter
- prior knowledge - what the agent knows prior to learning (this chapter assumes little-to-0 pk)
- transfer learning - knowledge from one domain is transferred to a new domain, so that learning can proceed faster with less data
- induction - going from a specific set of observations to a general rule
- deduction - conclusions are guaranteed to be correct if the premises are correct (more on this in Chapter 7.
- factored representation - a vector of the attribute values are used as inputs, representing the problems abstractly
- types of learning problems
	- classification - when a learning problem is designed around the output being a finite set of values
	- regression - when the learning problem is designed around the output being an integer or real value (typically floating point). A better name for this type of learning model would be "function approximation" or "numeric-prediction".
- types of feedback
	- supervised learning
		- agent observes input-output pairs
		- agent learns a function which maps from input to output
		- the output is called a **label**
	- unsupervised learning
		- the agent learns patterns in the input without any explicit feedback
		- the most common unsupervised learning task is **clustering**, detecting potentially useful clusters of input examples
	- reinforcement learning
		- the agent learns from a series of reinforcements, rewards and punishments

## Supervised Learning
> more formally, the task of supervised learning is this

- Given a **training set** of $N$ example input-output pairs: $(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)$
- each pair was generated by an unknown function: $y=f_{?}(x)$
- discover a function $h$ such that $h(x) \approx f_?(x)$ for all $x$

The function $h$ is called a **hypothesis**. Many ML nerds refer to hypotheses and sets of hypotheses when talking about learning models.

Oh and here it is.

> It is drawn from a **hypothesis space** $H$ of all possible functions. For example, the hypothesis space might be the set polynomials of degree 3, or the \[complete\] set of Javascript functions \[which have no compilation errors\], or the set of 3-SAT boolean logic formulas.

- $h$ is a **model** of the data,
	- drawn from a **model class** $H$
	- alternatively, a **function** drawn from a **function class**
- the output $y_i$ is the **ground truth** - the true answer we are asking our model to preduct

> How do we choose a hypothesis space?

We need to roll up our sleeves and do some work with some **exploratory data analysis**

- What process generated the example data?
- perform statistical tests
- generate data visualizations
	- histograms
	- scatter plots (2D, 3D, 3.5D)
	- box plots
- try multiple hypotheses and see what works better

> How do we choose a good hypothesis from within the hypothesis space?

- we hope for a **consistent hypothesis**
- $h$ such that $h(x_i) = y_i$ every time
- with continuous valued outputs we can expect: $h(x_i) = y_i \ne y_{gt,i} = f(x_i)$ where $gt$ means "ground truth"
- we hope for $h(x_i) = y_i \approx y_{gt,i} = f(x_i)$ with continuously valued $y$
- This hope is defined as looking for a **best-fit function**
- We say that $h$ **generalizes** well if it accurately predicts the outputs of the test set.

> The true measure of a hypothesis is how well it handles inputs it has not seen yet.

![[Pasted image 20240309122014.png]]

- each plot shows a function $h$ that a learning algorithm discovered depending on the hypothesis space $H$
- each column is a separate $H$
- each row has the same set of training data

- column 1 - straight lines - there is no lone that would be a consistent hypothesis for the data points. Probably not overfit.
- column 2 - sinusoidal functions - not quite consistent, but fits the data well. probably about right for the problem.
- column 3 - piecewise linear functions - each line segment connects the docs from one data point to the next. consistent. definitely overfit.
- column 4 - degree 12 polynomials - consistent. definitely overfit.

two ways to analyze hypothesis spaces
- by the bias they impose, regardless of the training data set
- by the variance they produce from one training set to another
- often there is a tradeoff between bias and variance, choosing between
	- more complex, low-bias hypotheses
	- less complex, low-variance hypotheses
- always that the overall goal is GENERALIZATION. Often the simpler models are better, even if they result in slightly higher biases over more complicated models.

### Bias
- the tendency of a predictive hypotheses to deviate from the expected value when averaged over different training sets
- bias often results from restrictions imposed by the hypothesis space
	- hypotheses from $H$ of linear functions induces strong biases 
	- any patterns that cannot be explained by the simple slope of a line, a linear function cannot represent those patterns
- A hypothesis is **underfitting** when it fails to find a pattern in the data

### Variance
- the amount of change in the hypothesis due to fluctuation in the training data
- a function is **overfitting** the data when it pays too much attention to the particular data set it is trained on, causing it to perform poorly on testing sets and other data not yet gathered/seen

### Complexity
- defining simplicity is not easy
- with polynomials, it's clear that only 2 params is less complex compared to 13
- Refer to Chapter 21, which covers deep neural network models, which are incredibly complicated, but generalize well.
- We should aim for "appropriateness" not "simplicity" in a model class
- If we know how the data was generated, we may get some insight into which model would be more appropriate
	- time based and referring to a human activity? cyclical for some other reason? sinusoidal might be more appropriate
	- high noise? linear functions might be more appropriate

### Probability
- supervised learning can be done by choosing the "most probable" hypothesis $h^*$
- $h^* = argmax_{h \in H}P(h|data)$
- By bayes rule, this is equivalent to:
- $h^*=argmax_{h \in H}P(data|h)P(h)$
- The prior probability $P(h)$ is high for a smooth degree 1 or 2 polynomial and lower for a degree-12 polynomial.
- we discourage more complex methods by giving them a lower probability
- We don't let $H$ be a space with too many parameters, such as the space of all Python 3.12 functions containing 12-or-fewer expressions and only use functions from the standard library, because

> there is a tradeoff between the expressiveness of a hypothesis space and the computational complexity of finding a good hypothesis within that space.

There is also a tradeoff with how long it takes to execute $h$ for a given $x$. The set of all python functions is guaranteed to contain functions which have terrible performance.

- most work on learning is focused on simple representations
- there has been a lot of interest in deep learning, where representations are not simple, but the cost of $h(x)$ is bounded by $O(1)$ on appropriate hardware

## 19.3 Learning Decision Trees
Refer to [[mitchell-decision-trees.pdf]] and [[Mitchell - Decision Trees]] for more comprehensive notes on decision trees.

![[Pasted image 20240309130630.png]]

![[Pasted image 20240309131010.png]]

Decision trees are equivalent to a logical disjunction of conjunctions.
- $Output = (Path_1 \lor Path_2 \lor ...)$
- $Path_i=(A_m=v_x \space\land\space A_n=y_y \space\land\space ...)$

> It is intractable to find a guaranteed smallest consistent tree. With some simple heuristics, we can find one that is close to smallest, fairly efficiently.

- tree structure, no loops, no backtracking
- no attribute is reviewed more than once per path
- can only classify data with rectangular, axis-aligned boxes, so it can't easily represent data which has diagonal decision boundaries

we can evaluate the performance of a learning algorithm with a **learning curve**
- split the data into a training and a testing set. We probably also want a 2nd testing set, skimmed off the top at the very beginning.
- Generate a hypothesis $h$ from the training set and determine its error/accuracy using the testing set
- try different split sizes for the training and testing sets. Generally the accuracy improves if you have more training data.
- we call these "happy graphs" if the accuracy continues to improve given more training data

![[Pasted image 20240309131724.png]]

- learning tree algorithms generally pick the attribute which gives the most information gain within the testing set at each decision point in the tree.
- information gain is a function of entropy
- see the mitchell notes for more info on how $Gain$ is calculated: [[Mitchell - Decision Trees]]

- we prevent overfitting by pruning the tree. There's many options for this.

## 19.4 Model Selection and Optimization
- Assume that future examples are like the past. This is the **stationary** assumption: $P(E_j) = P(E_{j+1})=P(E_{j+2})=...$
- Assume that all examples are independent of the previous examples: $P(E_j)=P(E_j \space|\space E_{j-1},\space E_{j-2},\space ...)$
- Examples that satisfy these probability constraints are independent and identically distributed (i.i.d.)
- The optimal fit of a model is the hypothesis that minimizes the error rate
	- the error rate is the proportion of times that $h(x) \ne y$ for an $(x,y)$ ground truth example, where classifications are a discrete set of values.
	- There's more complicated error rate calculations when outputs are not discrete.
- We estimate the error rate of a hypothesis by measuring its performance on a test set of examples
- Often we will end up creating multiple hypotheses
	- try different hypytheses from different $H$
	- try different parameters for hypotheses within the same $H$
	- Example, we could try different thresholds for $\chi^2$ pruning of decision trees, or different degrees for polynomials.
	- **hyperparameters** are parameters of the model class $H$, not the individual model
- The important thing is making sure that each individual hypothesis doesn't peek at the testing set. When comparing models, we can peek at the test set, but the test set should not be considered when generating each individual model.
- Typically we need 3 sets of data
	- A training set to train candidate models/hypotheses
	- A validation set, also known as a development/dev set, for evaluating candidate models and choosing the best one
	- A test set to do a final unbiased evaluation of the best model.
- If you don't have enough data to make all 3 of these datasets, you can use k-fold cross-validation
	- each example serves double duty as a training and validation data point, probibilistically
	- We split the data into $k$ equal subsets.
		- On each round, $1/k$ of the data is held out as a validation set.
		- The remaining examples are used as the training set.
	- You can also just randomly select $1/k$ examples as the validation set each iteration. Since the problem is that the data is few, you can run many more trails that you otherwise would be able to.
	- Popular values for k are 5 to 10.
	- An extreme example is leave-one-out cross-validation LOOCV, in which the test set is always just one datapoint.
- Part of model selection is qualitative and subjective as well. You're not going to try out models that you don't really understand. You're not going to try out models from hypotheses spaces that historically haven't fit the kind of problem you're currently working on. Having biases makes the work more efficient.

### Overfitting, Complexity, Interpolation
- 2 typical patterns occur in model selection
	- the training set error decreases monotonically as we increase the complexity of the model
	- For many model classes, the training set error reaches 0 as the complexity increases.

![[Pasted image 20240309144759.png]]

- Different model classes have different ways of handling excess capacity
- As we add capacity to a model class, we reach the point where all of the training examples can be represented perfectly within the model
- Given a training set with $n$ distinct examples, there is always a decision tree with $n$ leaf nodes that can represent all the examples.
- A model that exactly fits the training data has **interpolated** the data
- Model classes typically start to overfit as the capacity approaches the point of interpolation.

> That seems to be because most of the modelâ€™s capacity is concentrated on the training examples, and the capacity that remains is allocated rather randomly in a way that is not representative of the patterns in the validation data set. Some model classes never recover from this overfitting, as with the decision trees.

### Error and Loss
- So far the goal has been minimizing error rates
- Consider that not all error is created equal
- Email example
	- classifying an important email as spam is worse than not classifying a genuinely spam message as spam
	- A classifier with a 1% error rate where almost all of the errors were classifying spam as not-spam could be considered better than a classifier with a 0.5% error rate for both spam and not spam.
- In ML, it is traditional to minimize a **loss function** rather than maximize a utility function.
	- $L(x,y,\hat{y})$
	- It's the amount of utility lost by predicting $h(x)=\hat{y} \ne y = f(x)$
	- $L(x,y,\hat{y})=Utility(y \space|\space x)-Utility(\hat{y} \space|\space x)$
	- $Utility(y \space|\space x)$ is the utility of the result being $y$ given an input $x$
- This is the general formulation. Often the simplified version $L(y,\hat{y})$ is used, independent of $x$.
	- We can't say that it's worse to classify a genuine letter from Mom than it is to misclassify a genuine letter from an annoying coworker.
	- We can say that it's 10x worse to classify non-spam as spam.
		- $L(spam, notspam)=1$ - "if something is spam and we classify it as not spam, the loss is 1"
		- $L(notspam, spam) = 10$ - "if something is not spam and we classify it as spam, the loss is 10"
- $L(a,a)$ is always $0$.
- Loss can have many different definitions
	- Absolute difference loss: $L_1(y,\hat{y})=|y-\hat{y}|$
	- Squared error loss: $L_2(y,\hat{y})=(y-\hat{y})^2$
	- 0/1 loss: $L_{0/1}(y,\hat{y})=0 \space if \space y = \hat{y},\space else \space 1$
- The **generalization loss** for a hypothesis $h$ (with respect to the loss function $L$ is) is:
$$
GenLoss_L(h)=\sum_{(x,y) \in \epsilon} L(y,h(x))P(x,y)
$$
- You need to know the prior probability distribution $P(X,Y)$ over the examples in order to calculated $GenLoss_L(h)$
- The best hypothesis $h^*$ is the one with the minimum expected generalization loss
- $h^*=argmin_{h \in H}GenLoss_L(h)$
- $P(x,y)$ is not known in most cases. If that's true, we use empirical loss on a set of examples $E$ of size $N$.
$$
EmpLoss_{L,E}(h)=\sum_{(x,y)\in \epsilon}L(y,h(x))\frac{1}{N}
$$
- Our estimated best hypothesis $\hat{h}^*$ is the one with the minimum empirical loss
- $h^*=argmin_{h \in H}EmpLoss_{L,E}(h)$

### Estimated Best Hypothesis differs from True Function
There's 4 main reasons for $\hat{h}^* \ne f$

- unrealizability
	- a learning problem is realizable if if the hypothesis space $H$ contains $f$
	- if $H$ is linear functions, and $f$ is quadratic, no amount of data will result in an $h$ such that $h=f$
- variance
	- a learning algorithm will in general return different hypotheses for different sets of examples
	- if the problem is realizable, variance decreases to zero as the number of training examples increases
- noise
	- $f$ may be nondeterministic or otherwise noisy
	- deterministic noise: $f(x)=x+random(seed=x+C, range=(-1,1))$
	- nondeterministic noise is the most likely $f_{t=0}(x) \ne f_{t=1}(x)$
	- noise cannot be predicted
- computational complexity
	- when $H$ is a complicated function in a large hypothesis space, it can be **computationally intractable** to search all possibilities
	- a search can explore part of the space and return a reasonably good hypothesis, but can't always guarantee the best one.

### Regularization
- search for a hypothesis that directly minimizes the weighted sum of empirical loss and the complexity of the hypothesis
- $Cost(h)=EmpLoss(h)+\lambda\space Complexity(h)$
- $\hat{h}^*=argmin_{h\in H}Cost(h)$
- $\lambda$ is a hyperparameter, a positive number that serves as a conversion rate between loss and hypothesis complexity
- The choice of regularization function depends on the hypothesis space.
- For polynomials, a good regularization function is the sum of the squares of the coefficients
- Another way to simplify models is to reduce the dimensions that the models work with. A process of **feature selection** can be performed to discard attributes that appear to be irrelevant.
- $\chi^2$ pruning is a kind of feature selection
- It's possible for the empirical loss and complexity measured to be on the same scale with out a conversion factor $\lambda$
	- We can measure both in terms of bits
	- This comes from information theory
	- The **minimum description length** or MDL hypothesis minimizes the total number of bits required
	- More on this in Chapter 20

### Hyperparameter Tuning
i it can be hard to choose good values for hyperparameters
- when there are multiple hyperparams
- when the hyperparams have continuous values

simplest approach is **hand tuning**
- guess some good parameter values based on past experience
- train
- measure
- analyze
- repeat until bored or budget exhausted

Only a few hyperparams each with a small number of possible values, try a grid search
- try all combinations of values to see which performs best
- different combinations can be tested in parallel
- time to complete is dependent on available resources and credit card limit
- If 2 hyperparameters are independent of each other, they can be optimized separately
- If there are too many combinations of possible values
	- take random search samples uniformly across the space of hyperparameter settings
	- this also works when there are continuously valued hyperparameters

**bayesian optimization**
- when each training run takes a long time, it can be helpful to get information out of each run
- treats the task of selecting hyperparameter values as an ML problem
- $x$ = the vector of hyperparameter values as a vector
- $y$ = the total loss on the validation set for the model built with hyperparameters $x$
- tries to find the function $f(x)=y$ that minimizes loss $y$
- Each time we do a training run, we get a new $(y, f)$ pair, which we can use to update out belief about the shape of function $f$

**Gaussian process**
- We can trade off exploitation with exploration
	- exploitation - trying out parameters that are near to a previous good result
	- exploration - trying out a novel set of parameters
- same tradeoff made in other areas of this textbook, such as monte-carlo tree search
- the idea of upper confidence bounds is used here to minimize regret
- if $f$ can be approximated by a Gaussian process, then the math of updating our belief about $f$ works nicely

**population-based training (PBT)**
- this is a genetic algorithm that combines parallelization with bayesian optimization
- PBT starts off using random search to train (in parallel) a population of models, each with different hyperparameter values
- a second generation of models are trained using hyperparameter values close to the values from the previous generation
- adding random mutation

## 19.5 The Theory of Learning
Key terms and section headers
- computational learning theory
- probably approximately correct (PAC)
- PAC learning
- approximately correct
- sample complexity
- PAC learning example - learning decision lists
	- decision lists

## 19.6 Linear Regression and Classification
(Skipped for now.)

## 19.7 Nearest Neighbors Models
- given an known datapoint $x_q$ ($q$ meaning query), we can classify it based on he $k$ examples that are nearest to $x_q$.
- This is called k-nearest neighbors lookup (KNN)
- Notation $NN(k,x_q)$
- Take the most common classification from the output of $NN$ to determine the classification for $x_q$
- We do not add $x_q$ to the examples without some other form of confirmation
- $k$ is usually set to an odd value when classification is binary to avoid ties. In general, $k$ should be set to avoid ties, which is harder as the number of valid classifications increases.
- For regression (continuously valued classification problems), we can take the mean or median of the $k$ neighbors

![[Pasted image 20240309164329.png]]

The distance from a query point $x_q$ to an example point $x_j$ is typically measured with a **Minkowski distance**
$$
L^P(x_j,x_q)=\left(\sum_i | x_{j,i} - x_{q,i} |^p \right)^{1/p}
$$
- with $p=2$, this is euclidean distance
	- useful if the dimensions are measuring similar properties (width, height, depth)
- with $p=1$, this is manhattan distance
	- useful if attributes are dissimilar (age, weight, gender)
- with boolean attribute values, the number of attributes on which the 2 points differ is called the **hamming distance**
- note that the units of each attribute affects the distance
- how do we compare a difference in age to a difference in weight?
- common to apply **normalization** to the measurements in each dimension
	- compute the mean $\mu_i$ and standard deviation $\sigma_i$ of the values in each dimension $i$
	- rescale the values using $(x_{j,i}-\mu_i)/\sigma_i$
- A complex metric known as **Mahalanobis distance** which takes into account the covariance between dimensions

k-nearest works best in low-dimension spaces with plenty of data. k-nearest starts to break down in higher dimensions, as the average distance between points in the data space increases.

This is called the **curse of dimensionality**

Also note that basic KNN has trouble when the number of examples $N$ increases, as we need to compare each query $x_q$ with all of the examples. This requires a scheme for partitioning the examples so that we can compare $x_q$ with some number of examples $E_{x_q}$ to find its $k$ nearest examples, such that $k \le |E_{x_q}| \lt \lt |E|$.

### k-d trees
> A balanced binary tree over data with an arbitrary number of dimensions is called a **k-d tree**, for k-dimensional tree.

- similar to the construction of a balanced binary tree.
- we start with a set of examples and at the root node we split them along the i-th dimension by testing whether $x_i \le m$, where $m$ is the median of the examples. Half the examples should end up in the right branch, the other half should end up in the left branch.
- We recursively make a tree for the left and right sets of examples, stopping when there are fewer than 2 examples left
- To choose a dimension to split on at each node of the tree, one can simply select dimension $i \space mod \space n$ at level $i$ of the tree.
- We may need to split on any given dimension several times as we proceed down the tree.
- Another strategy is to split on the dimension that has the widest spread of values.
- Lookup
	- Exact lookup from a k-d tree is just like lookup from a binary tree
	- The complication is that you need to pay attention to which dimension you are testing at each node
	- NN lookup further more complicated by the fact that you can't always ignore half of the examples when traversing the tree.
	- When traversing this tree with $x_q$, we may need to back-track in order to find all of the $k$ -nearest nodes to $x_q$.
- k-d trees are only appropriate when there are many more examples than dimensions, preferably when $|E| \ge 2^n$.

### Locality-sensitive hashing (LSH)
- this describes a different algorithm called approximate-nearest neighbors (ANN)
- effectively, we define a data structure which "probably" finds the $k$ nearest neighbors and is capable of reporting a failure when there are no examples within a radius $r$ of $x_q$
- If there is a point $x_j$ within a radius $r$ of $x_q$, then with probability $p$, the algorithm will find a point $x_{j'}$ that is within $cr$ of $x_q$
- $c$ and $p$ are hyperparameters of the algorithm
- for ANN, we need a LSH hash function $g(x)$ that produces the same hash with a low probability if $d(x_j, x_{j'}) \ge cr$ and a high probability if $d(x_j,x_{j'}) \lt r$
- This results in many different "hash buckets" that are created by $g$
- LSH typically uses multiple $g$ definitions to create multiple sets of hash buckets
- On query, ANN gets all of the candidate examples by union-ing the results of passing $x_q$ into all $g$: $Candidates(x_q)=g_1(x_q) \space\cup\space g_2(x_q) \space\cup\space ... \space\cup\space g_n(x_q)$
- From the candidates union set, ANN will compare the distances between $x_q$ and all of the candidates, taking the $k$ smallest, returning a failure if there are fewer than $k$ candidates

### Nonparametric regression
- known informally as "connect-the-dots"
- known superciliously as "piecewise-linear nonparametric regression"
- interpolates between adjacent examples using some interpolation scheme, of which there are many
- parameters include the interpolation scheme, and parameters of each interpolation scheme
- model accuracy is determined using cross-validation

![[Pasted image 20240309191146.png]]

- locally weighted regression gives the advantage of nn without the discontinuities
- each example is weighted using a **kernel**

### Support vector machines
- this was the most popular approach for "off-the-shelf" supervised learning in the early 2000s
- great for when you don't have any specialized prior knowledge about the domain
- overtaken by deep learning networks and random forests
- still has 3 nice properties
	- construct a max-margin separator - a decision boundary with the largest possible distance to example points
	- creates a linear separating hyperplane, and have the ability to embed the data into a higher-dimensional space, using a kernel trick. data that isn't linearly separable in a lower dimensional space are separable in a higher dimensional space
	- non parametric, can represent complex functions, but are resistant to overfitting

![[Pasted image 20240309192506.png]]

- attempts to minimize generalization loss
- assumes that future datapoints are drawn from the same distribution as the current set of examples
- minimizes generalization loss by choosing the decision boundary which is furthest away from examples seen so far
- This is the **maximum margin separator**
- the **margin** is the width of the area bounded by dashed lines in the future above, twice the distance from the separator to the nearest example point
- uses a quadratic programming optimization problem
- existing software does this quite well

![[Pasted image 20240309192921.png]]

### The Kernel Trick
- in the case of noisy data, we may prefer a "decision surface in a lower dimensional space" to a "linear separator in a higher dimensional space"
- soft margin classifier which allows examples to fall on the wrong side of the decision boundary

## 19.8 Ensemble Learning
(skipped for now)

## 19.9 Developing Machine Learning Systems
(skipped for now)