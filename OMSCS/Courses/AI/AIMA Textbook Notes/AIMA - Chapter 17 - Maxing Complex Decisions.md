---
tags:
  - OMSCS
  - AI
  - AIMA
---
# AIMA - Chapter 17 - Maxing Complex Decisions

Dear reader,

The final opens up sometime this week. I've been taking diligent notes since January. I'm feeling pretty burned out. These notes will pretty much just contain some high-level information about what's in the chapter, and how it relates to Module 10, which I also didn't really take notes on.

I hope you understand.

## Notes

- This chapter contains a lot of analysis into the grid world presented by Thrun in Module 10

![[Pasted image 20240424082800.png]]

![[Pasted image 20240424082829.png]]

![[Pasted image 20240424082917.png]]

## Algorithms for MDPs
- value iteration
	- bellman update
- policy iteration
	- policy evaluation
	- policy improvement
- linear programming (LP)
- Online algorithms for MDPs
	- real time dynamic programming (RTDP)

## Bandit Problems
> In Las Vegas, a one-armed bandit is a slot machine.

- exploration vs exploitation
- markov reward process (MRP)
- This whole section is about observing slot machines and trying to craft a reward function. The real (only) reward is dopamine.

## Partially Observable MDPs
> POMDPs

This wasn't really covered in the module, apart from 2 short videos containing a 30,000ft overhead view. Hopefully it's not on the final.